# 卷积神经网络

1. 卷积神经网络`convolutional neural network:CNN`：专门处理具有类似网络结构的数据的神经网络

   - 如：时间序列是一维网格；图像数据是二维网格

   - 卷积网络是指那些至少在网络的某一层中使用了卷积运算来代替一般的矩阵乘法运算的神经网络

## 1. 卷积运算

1. 卷积示例：一个激光传感器给出输出 $x(t)$ 表示宇宙飞船在时刻 $t$ 的位置

   - 假设传感器包含噪声，那么我们希望利用测量结果的均值来估计飞船的位置

   - 我们认为：越近的测量结果越相关，于是对最近的测量结果赋予更高的权重

   - 令 $w(a)$ 为权重函数，其中 $a$ 表示测量结果距离当前时刻的间隔，则得到飞船位置的估计：

     $$
      s(t)=\int x(a)w(t-a)da
     $$

   - 这种运算就称作卷积`convolution`，用符号星号 $s(t)=(x*w)(t)$ 表示

     > 理论上 $w(\cdot)$ 可以为任意的实值函数。但是这里我们要求：
     >
     > - $w(\cdot)$ 是个有效的概率密度函数。否则输出就不是一个加权平均
     > - $w(\cdot)$ 在自变量为负数时，取值为零。否则涉及到未来函数(实际上我们只能观测到过去的结果)

2. 在卷积神经网络中，函数 $x(\cdot)$ 称作输入；函数 $w(\cdot)$ 称作核函数。而输出有时被称作特征映射 `feature map`

3. 通常当计算机处理数据时，连续的数据会被离散化。因此时间 $t$ 只能取离散值。

   假设 $x(\cdot),w(\cdot)$ 都是定义在整数时刻 $t$ 上，那么得到了离散形式的卷积：

   $$
   s(t)=(x*w)(t)=\sum_{a=-\infty}^{a=\infty}x(a)w(t-a)
   $$

   - 实际操作中，我们只能存储有限的数据，因此这些函数的值在有限的点之外均为零。因此无限求和最终是有限的求和

4. 我们可以对多个维度进行卷积运算。如果二维图像 $\mathbf I$ 作为输入，那么我们需要使用二维的核函数 $\mathbf K$，则输出为：

   $$
   \mathbf S(i,j)=(\mathbf I*\mathbf K)(i,j)=\sum_m\sum_n\mathbf I(m,n)\mathbf K(i-m,j-n)
   $$

   由于卷积是可交换的，因此等价写作：

   $$
   \mathbf S(i,j)=(\mathbf I*\mathbf K)(i,j)=\sum_m\sum_n\mathbf I(i-m,j-n)\mathbf K(m,n)
   $$

   这称作我们相对输入翻转`flip`了核。

   > 卷积的可交换性在数学证明中有用，但是在神经网络应用中不是一个重要的性质

5. 许多神经网络库会实现一个相关的函数，称作互相关函数`cross-correlation`。它类似于卷积：

   $$
   \mathbf S(i,j)=(\mathbf I*\mathbf K)(i,j)=\sum_m\sum_n\mathbf I(i+m,j+n)\mathbf K(m,n)
   $$

   - 有些机器学习的库将它称作卷积。这里我们遵循传统，也将它称作卷积。只有在明确核的翻转这个上下文时，才特别指明区分

6. 在机器学习中，学习算法会为核函数在合适的位置学得恰当的值

   - 一个基于核翻转的卷积运算的学习算法学得的核，是采用未翻转的算法学得的核的翻转

7. 单独使用卷积运算在机器学习中很少见，卷积通常和其他函数一起使用

   - 无论卷积运算是否翻转了核，这些函数的组合通常不可交换

8. 一个2维卷积的例子（核未翻转）：

   ![2d_con](../imgs/9/2d_con.png)

   理论上核需要翻转（卷积的定义）。但是这里未翻转核，因此采用的是

   $$
   \mathbf S(i,j)=(\mathbf I*\mathbf K)(i,j)=\sum_{m=0}^{1}\sum_{n=0}^{1}\mathbf I(i+m,j+n)\mathbf K(m,n)
   $$

   -  $m$ 和 $n$ 由核函数决定（这里分别为  [0,2),[0,2) ）

9. 离散卷积可以看做矩阵的乘法，但是该矩阵的一些元素被限定为：必须和另一些元素相等

   - 对于单变量的离散卷积，矩阵的每一行必须和上一行移动一个元素后相等，这种矩阵叫做`Toeplitz`矩阵
   - 对于二维的情况，卷积对应着一个双重块循环矩阵。除了元素相等的限制以外，卷积通常对应了一个非常稀疏的矩阵（几乎所有元素都为零）

10. 一个使用矩阵乘法但是不依赖矩阵的结构的特殊性的神经网络算法，都适用于卷积运算，且不需要对神经网络作出大的修改。因为卷积可以转换成矩阵乘法

11. 卷积和矩阵乘法：

    - 循环矩阵的定义：

      $$
      \mathbf C=\begin{bmatrix}c_0&c_{n-1}&\cdots&c_2&c_1\\c_1&c_0&\cdots&c_3&c_2\\ 
      \vdots&\vdots&\ddots&\vdots\\
      c_{n-1}&c_{n-2}&\cdots&c_1&c_0\end{bmatrix}
      $$

    - 我们可以利用循环矩阵求卷积

    - 假设有两个长度分别为 $M$ 和 $N$ 的序列 $x(i)$ 和 $w(i)$， 则卷积 

      $$
      s(i)=x(i)*w(i)=\sum_{j}x(j)w(i-j)
      $$

      卷积的长度为 $L=M+N-1$。 

      首先用 0 扩充序列 $x,w$:

      $$
      x_p(i)=\begin{cases}x(i)&,1\le i\le M\\ 0&, M\lt i\le L\end{cases}
      $$

      $$
      w_p(i)=\begin{cases}w(i)&,1\le i\le N\\ 0&, N\lt i\le L\end{cases}
      $$

      由于用 $w$ 取卷积 $x$，因此构造 $w$ 的循环矩阵：

      $$
      \mathbf W=\begin{bmatrix}w_p(1)&w_p(L)&w_p(L-1)&\cdots&w_p(2)\\ 
      w_p(2)&w_p(1)&w_p(L)&\cdots&w_p(3)\\ 
      \vdots&\vdots&\vdots&\ddots&\vdots\\
      w_p(L)&w_p(L-1)&w_p(L-2)&\cdots&w_p(1) \end{bmatrix}
      $$

      > 这里列优先，因此第一列是完全顺序的

      则卷积为：

      $$
      s=\mathbf W\cdot x_p= \begin{bmatrix}w_p(1)&w_p(L)&w_p(L-1)&\cdots&w_p(2)\\ 
      w_p(2)&w_p(1)&w_p(L)&\cdots&w_p(3)\\ 
      \vdots&\vdots&\vdots&\ddots&\vdots\\
      w_p(L)&w_p(L-1)&w_p(L-2)&\cdots&w_p(1) \end{bmatrix}\cdot \begin{bmatrix}x_p(1)\\
      x_p(2)\\ 
      \vdots\\
      x_p(L)\end{bmatrix}
      $$

12. 二维卷积：$\mathbf S(i,j)=(\mathbf I*\mathbf K)(i,j)=\sum_m\sum_n\mathbf I(m,n)\mathbf K(i-m,j-n)$ 

    - 先将 $\mathbf I,\mathbf K$ 扩充到 $M\times N$ 维。扩充之后的新矩阵为 $\mathbf I_p,\mathbf K_p$ 

    - 用 $\mathbf I_p$ 构造一个列向量 $f_p$ ：将 $\mathbf I_p$ 的第一行转置之后将其成为 $f_p$ 的前 $N$ 个元素；接下来是第二行的转置....第 $M$ 行的转置

    - 将 $\mathbf K_p$ 中的每一行，都按照一维卷积中介绍的循环矩阵生成的方法构成一个 $N\times N$ 的循环矩阵。这样的矩阵记做： $\mathbf G_1,\mathbf G_2,\cdots \mathbf G_M$ 

    - 用这些循环矩阵构造一个大的块循环矩阵

      $$
      \mathbf G_b=\begin{bmatrix}[\mathbf G_1]&[\mathbf G_M]&\cdots& [\mathbf G_2] \\ 
      [\mathbf G_2]&[\mathbf G_1]&\cdots &[\mathbf G_3]\\ 
      \vdots&\vdots&\ddots&\vdots\\ 
      [\mathbf G_M]&[\mathbf G_{M-1}]&\cdots&[\mathbf G_1]\end{bmatrix} 
      $$

    - 于是二维卷积可以表示为： $h_b=\mathbf G_b \cdot f_p $  ， 将 $h_b$ 的结果分配到 $\mathbf S$ 的各行（与构造 $f_p$ 相反的过程），即得到二维卷积

## 2. 动机

1. 卷积运算通过三个重要的思想来改进机器学习系统：
   - 稀疏交互`sparse interactions`
   - 参数共享`parameter sharing`
   - 等变表示`equivariant representation`
2. 一些不能被传统的神经网络处理的特殊数据可以通过卷积神经网络处理
   - 传统神经网络要求输入的数据是固定大小的

### 2.1 稀疏交互 

1. 稀疏交互：传统的神经网络是全连接的

   - 传统的神经网络用矩阵乘法来建立输入与输出的连接关系。矩阵的每个参数都是独立的，它描述了每个输入单元与输出单元的交互

   - 这意味着每个输出单元与每个输入单元都产生交互

   - 卷积神经网络通过使得核规模远小于输入规模，来实现稀疏交互（也称作稀疏连接，或者稀疏权重）

   - 这意味着我们需要存储的参数更少，这不仅减少了模型的存储需求，也降低了计算量

2. 稀疏交互：每个输入单元影响的输出单元

   ![sparse_1](../imgs/9/sparse1.png)  

   这里给出了从下往上看时，每个输入单元影响的输出单元。

   - 对于传统神经网络（下图），每个输入单元影响了所有的输出单元（以 $x_3$ 为例）
   - 对于卷积神经网络（上图），每个输入单元只影响了3个输出单元（核宽度为3时）（以 $x_3$ 为例）

3. 稀疏交互：每个输出单元依赖的输入单元

   ​	![sparse2](../imgs/9/sparse2.png) 

   这里给出了上往下看，每个输出单元依赖的输入单元

   - 对于传统神经网络（下图），每个输出单元依赖所有的输入单元（以 $s_3$ 为例）
   - 对于卷积神经网络（上图），每个输出单元只依赖3个输入单元（核宽度为3时）（以 $s_3$ 为例）

4. 稀疏交互：虽然每个输出单元只依赖于少量的直接输入单元，但是它可能间接依赖于大部分的间接输入单元

     ![sparse3](../imgs/9/sparse3.png)	

     ​	处在卷积网络更深层的单元，其接受域（即影响它的神经元）要比处在浅层的单元的接受域更大

### 2.2 参数共享

1. 参数共享：在模型的多个函数中使用相同的参数

2. 传统的神经网络中，当计算一层的输出时，权重矩阵的每个元素只使用一次（在与输入做矩阵乘法之后，再也用不着了）。

   - 而在卷积神经网络中，同一个核会在输入的不同区域做卷积运算。这意味着我们并不需要对于输入的每个位置来学习一个单独的参数（传统神经网络的做法），而是只需要学习核函数。

   - 核函数会在输入的不同区域之间共享

     > 这个区域的大小就是核函数的大小

3. 卷积神经网络显著地把模型的存储需求降低到 $k$ 个参数 ($k$ 为核矩阵的大小)，因此卷积在存储需求和计算效率方面要大大优于传统神经网络的稠密矩阵的乘法运算。

4. 下面给出了卷积神经网络和传统神经网络的参数

   - 上图中， $x_3\rightarrow s_3$ 的权重被共享到 $x_i\rightarrow s_i,i=1,2,4,5$ 
   - 下图中，  $x_3\rightarrow s_3$ 的权重只使用了一次

   ![share](../imgs/9/share.png) 

5. 卷积神经网络就是将小的、局部区域上的相同的线性变换（由核函数描述），应用到整个输入上。

### 2.3  等变表示

1. 等变：如果一个函数满足输入发生改变，输出也以同样的方式改变，则称它是等变的`equivariant`

2. 如果函数 $f(x),g(x)$ 满足 $f(g(x))=g(f(x))$ ，则称 $f(x) $ 对于变换 $g$ 具有等变性

3. 对于卷积神经网络，它具有平移等变的性质

   - 如果 $g$ 是输入的任何一个平移函数（比如说，将图片向右移动一个像素），则先对图片进行卷积之后再对结果应用平移函数 $g$ ，与先对图片应用平移函数 $g$ 再对图片进行卷积，结果是相同的

4. 当处理时间序列时，如果把输入中的一个事件向后延时，则卷积的输出中也发生相应的延时。

   处理图像也类似：如果移动了输入的对象，则卷积的输出结果也移动了相同的量

5. 卷积对于其它的一些变换并不是等变的：如缩放变换（改变图像的尺寸）、角度变换。

   - 这些非等变的变换需要其它机制来处理（而不是卷积）

## 3. 池化

1. 卷积神经网络的卷基层通常包含三级：

   - 第一级：并行地执行多个卷积运算（注意：卷积是一种线性变换）。卷积的作用是从输入中提取特征。不同的核函数提取了不同的特征
   - 第二级：执行非线性的激活函数（如`reLU`单元）（这一级是非线性变换）。这一级有时也被称作探测级。其作用是引入非线性。
   - 第三极：通过池化函数来调整卷基层的输出。池化也叫亚采样或者下采样。它降低了各个输出的维度，但保持大部分重要的信息。

   > 如：对于一个图形，如水平梯度的卷积核 
   >
   > $$\begin{bmatrix}-1&-2&-1\\ 0&0&0\\ 1&2&1\end{bmatrix} $$ ，
   >
   > - 第一级卷积：它相当于一个滤波器，检测到图像在水平方向上的变化。
   > - 第二级 `reLU`：这一步类似传统的神经网络
   > - 池化函数：就是将一个局部的数据块规约到一个数值，从而降低数据的维度

2. 对于卷基层的分层有两种观点：

   - 卷积神经网络有少量的卷基层，每个卷基层都是复杂的、由三级组成的（如左图所示）
   - 卷积神经网络由多个层组成，每一层都是简单的（如右图所示）

   ![layer](../imgs/9/layer.png) 

3. 池化：用一个矩阵区域内部的某个总体统计特征来代神经网络在该矩阵区域的输出

   - 最大池化：定义一个窗口，并从窗口内取出最大值
   - 均值池化：定义一个窗口，并从窗口内取出平均值

   其他常见的还有： $L^{2}$  范数以及基于中心像素距离的加权平均函数

4. 池化的优势：当输入做出了少量的平移时，池化能够帮助我们获得输出的近似不变性。

   即：当我们把输入平移一个微小的量时，经过池化函数的输出值并不会发生改变。

   - 下图的上半部分：给出了一个最大池化的输出。
   - 下半部分：当最大池化的输入右移了一个单位时，最大池化的输出。
   - 可以看到：当输入的所有值发生改变时，最大池化的输出只有一半的值发生了改变。这是因为：最大池化单元只对周围的最大值比较敏感，而不是对精确的位置敏感

   ![pooling](../imgs/9/pooling.png) 

5. 局部平移不变性是个很重要的性质：它可以表明：我们关心某个特征是否出现，而不关心它出现的具体位置。

   - 如判断是否人脸时，我们并不关心眼睛的位置。只需要知道一只眼睛在脸的左边；一只眼睛在脸的右边
   - 但是有些领域，特征的具体位置很重要。如判断两条线段是否相交时

6. 池化相当于是一个非常强的先验：卷基层学得的函数必须具有对少量平移的不变性。

   - 只有这个假设成立时，池化才能极大地提高网络的统计效率

7. 池化只能对空间的平移具有不变性，而无法对空间的旋转具有不变性。

   - 我们可以将空间的旋转的角度进行离散化，每个角度对应一个卷积，然后采用池化。这种做法也可以实现空间的旋转不变性

   下图中，使用三个过滤器（卷积实现的）和一个最大池化单元可以学的旋转不变性。当输入旋转某个角度时，对应的过滤器被激活。只要任意一个过滤器被激活，最大池化单元也相应被激活。

   > 池化的空间平移不变性是天然不变的。其他变换的不变性则必须采用这种多通道的方法来实现

   ![rotate_pool](../imgs/9/rotate_pool.png) 

8. 因为池化综合了全部邻居的反馈，因此可以使得池化单元的数量少于探测单元（即`reLU`单元，它也是池化层的输入级）

   下图中，`reLU`单元有6个，池的宽度为3，池之间的步幅为 2，导致池化单元的数量降到3个。

   - 池化减少了下一层的输入，提高了网络计算效率，减少了参数的存储需求

   ![pooling_num](../imgs/9/pooling_num.png)

9. 如果对不同大小的图像进行分类，那么要求分类层的输入必须是固定大小

   - 可以通过卷基层来实现。卷基层的输出就是分类层的输入
   - 卷基层的池化处理中，调整池化区域的大小来实现不同大小的图像经过卷基层之后生成同样数量的输出

## 4.卷积与池化作为一种无限强的先验

1. 先验有强弱之分：

   - 弱的先验具有较高的熵（即较大的不确定性），例如方差很大的高斯分布（方差无穷大就成了均匀分布）
   - 强的先验具有较低的熵（具有很小的不确定性），例如方差很小的高斯分布（方差很小意味着随机变量取值几乎确定）

   一个无限强的先验对于参数在参数空间的某些取值的概率是0，无论我们的数据是否支持参数取这些值

2. 卷积神经网络也可以看做是一个全连接网络，但是它的权值有一个无限强的先验：

   - 隐单元之间：所有隐单元与上一层连接的权重都是重复的（即不同的隐单元，其连接权重都是相同的）
   - 隐单元内部：在隐单元与上一层连接中，只有隐单元接受区域内的权重非零，大多数的权重都是零

   这个先验表明：卷基层学得的函数只包含了局部连接关系，并且具有平移等变性。

3. 池化也是一个无限强的先验：每个池化单元都具有对少量平移的不变性

4. 卷积与池化只有当先验的假设合理，且正确时才有用

   - 如果任务依赖于保存精确的空间信息，那么使用池化将增大训练误差
   - 如果任务需要对输入中相隔较远的信息进行合并，那么卷积所需要的先验可能就不准确（因为卷积拥有平移不变性，相隔的远近没有意义）

## 5. 基本卷积函数的变体

1. 带有单个核的卷积只能提取一种类型的特征。通常我们希望神经网络的一层能够提取多个特征，因此我们在卷积神经网络中，并行使用多个卷积

2. 通常卷积神经网络的输入不仅仅是二维的图像（黑白的），也可能是三维的索引：一个索引来标明不同的通道（如红绿蓝），另外两个索引标明在每个通道上的空间坐标。甚至是高纬的张量

   - 假设我们有一个4维的核张量 $\mathbf K$ ，每个元素是  $K_{i,j,k,l}$ ，代表了一对输出单元和输入单元的权重：

     - $i$ 表示输出单元位于 $i$ 通道

     - $j$ 表示输入单元位于 $j$ 通道

     - $k,l$ 表示通道中的偏移（如图像上的偏移）

       > 比如：$i=红色,j=绿色,k =0,l=0$  时，表示绿色通道中图像位置 $(0,0)$ 对于红色通道中图像位置 $(0,0)$ 的权重

   - 假设我们的输入由观测数据 $\mathbf V$ 组成，每个元素是 $V_{i,j,k}$ 

     - $i$ 表示输入单元位于 $i$  通道

     - $j,k$ 表示输入位于该通道的第 $j$ 行第 $k$  列

       > 如：彩色图像 (通道有三个：红绿蓝)。$j=0,k=0$ 可以表示成蓝色通道中，图片 $(0,0)$ 位置处的值（取值范围 $0.0 \sim 1.0$ ） 

   - 假定输出 $\mathbf Z$ 和输入 $\mathbf V$ 具有相同的形式。如果只涉及卷积而不翻转核（即：这里并不是使用卷积的数学定义），则：

     $$
      Z_{i,j,k}=\sum_l\sum_{m=1}^{M}\sum_{n=1}^{N}V_{l,j+m-1,k+n-1}K_{i,l,m,n}
     $$

     - 固定 $i$ ：表明输出时 $i$ 通道
     - 对 $l$ 遍历：表明评估所有其他通道对于通道 $i$ 的影响
     - $m,n$ 的遍历：类似于二维图像的卷积。其中 $M$ 为卷积核的宽度； $N$ 为卷积核的高度（当然二者也可以一个表示高度，另一个表示宽度）
     - 这里减 1，是因为代数系统中，下标从 1 开始

3. 如果我们希望跳过图片中的一些位置来降低计算开销（相应代价是提取特征没有那么好了），我们可以对卷积输出进行降采样。

   - 假设我们希望对输出的每个方向上，每隔 $s$ 个像素进行采样，则定义一个降采样卷积函数 $c$ ，使得

     $$
      Z_{i,j,k}=c(\mathbf K,\mathbf V,s)_{i,j,k}=\sum_l\sum_{m=1}^{M}\sum_{n=1}^{N}V_{l,(j-1)\times s+m,(k-1)\times s+n}K_{i,l,m,n}
     $$

   - 这里 $s$ 称作降采样卷积的步幅。可以对不同的方向定义不同的步幅

     ![down_sampling](../imgs/9/down_sampling.png) 

     上图为降采样卷积的两种实现形式

     - 上半部分：通过直接实现步幅为2的卷积
     - 下半部分：先进行卷积，再降采样。这种做法会造成计算上的大量浪费

4. 在任何卷积神经网络的应用中都有一个重要性质：能隐含地对输入 $\mathbf V$ 进行用零填充，使得它加宽

   - 如果没有这个性质，那么每一层的宽度会逐层递减。根据卷积的性质，每一层宽度减少的数量等于核的像素减1.
     - 如果核较大，则网络空间的宽度迅速缩减。限制了网络的深度
     - 如果核较小，则可用的核的数量大幅度降低，网络的表达能力降低

5. 有三种填充零的方式：

   - 第一种：不使用零来填充，卷积核只允许访问那些图像中能完全包含整个核的位置。

     - 在`matlab`中，这称作有效卷积
     - 此时输出的数量等于输入的数量。但是输出的大小在每一层都缩减。如果输入图像的宽度是 $m$ ，核的宽度是 $k$ ，则输出的宽度变成 $m-k+1$ 。
     - 如果核的宽度非常大时，缩减非常明显。最终网络的空间维度会缩减到 $1\times 1$ 

     ![no_padding](../imgs/9/no_padding.png) 

   - 第二种：只进行足够的零来填充来保持输出和输入具有相同的大小

     - 在　`matlab` 中，称作相同卷积
     - 此时网络可以包含任意多的卷基层
     - 但是，输入像素中，靠近边界的部分相比中间部分来讲，它们对于输出像素的影响更小了。这可能导致边界像素存在一定程度的欠表达

     ![same_padding](../imgs/9/same_padding.png) 

   - 第三种：进行足够多的零来填充，使得每个像素在每个方向上恰好被访问了 $k$ 次（在输入的两端各填充 $k-1$ 个零，使得输入的每个像素都恰好被核访问 $k$ 次，最终输出宽度为 $[m+2(k-1)]-(k-1)=m+k-1$ 次）

     - 在`matlab` 中，称作全卷积
     - 最终输出图像的宽度为 $m+k-1$ 
     - 此时，输出图像的边界部分，相对于输出图像的中间部分，依赖于更少的输入像素。这回导致学习一个对所有位置都表现不错的单核更为困难

     通常零填充的最优数量处于有效卷积核相同卷积之间

6. 局部连接与卷积很类似：如下图所示，它也是连接受限的，但是每个连接都有自己的权重（并没有共享参数）

   ![local_join](../imgs/9/local_join.png) 

   我们用6维的张量 $\mathbf W$ 表示，元素 $w_{i,j,k,l,m,n}$  的索引表示：

   - $i$ 为输出的通道（如红色通道）
   - $j,k$ 为输出通道中，图像的坐标（行、列）
   - $l$ 为输入的通道（如绿色通道）
   - $m,n$ 为输入通道中，图像的坐标（行、列）

   于是局部连接层的线性部分表示为：

   $$
   Z_{i,j,k}=\sum_{l}\sum_m\sum_n[V_{l,j+m-1,k+n-1}w_{i,j,k,l,m,n}]
   $$

   > 因为下标从1计数，所以这里有 $-1$ 

   这称作非共享卷积，因为它并不横跨位置来共享参数。下图给出了局部连接、卷积、全连接的区别。从上到下依次为：局部连接、卷积、全连接


   ![local_join2](../imgs/9/local_join2.png) 

7. 如果我们知道特征是一小部分区域的函数，而不是整个区域的函数时，局部连接层很有用。此时我们只需要处理部分输入即可

   - 如果我们需要辨别一张图片是否人脸图像，则只需要在图像的下部中央部分寻找即可

8. 有时候，我们会进一步限制卷积或者局部连接层。

   - 我们可以限制每个输出的通道 $i$  仅仅是输入通道 $l$ 的一部分的函数（而不是通道 $l$ 的整体的函数）。这种方案减少了通道之间的连接，使得模型需要更少的参数，降低了存储的消耗，减少了计算量

9. 拼接卷积`tiled convolution` 对卷积层和局部连接层进行了这种：我们学习一组核，使得当我们在空间移动时，它们可以循环利用

   ![tiled_conv](../imgs/9/tiled_conv.png) 

   - 这意味着在近邻的位置上拥有不同的过滤器，就像局部连接层一样
   - 但是参数的存储需求仅仅会增长常数倍（该常数就是核的集合的大小），而不是整个输出的大小

   下图从上到下依次为局部连接、拼贴卷积、标准卷积

   ![tiled_conv2](../imgs/9/tiled_conv2.png) 

   令 $\mathbf K$ 为一个 6 维的张量，其元素为 $K_{i,j,k,l,m,n}$ ，索引表示：

   - $i$ 为输出的通道（如红色通道）
   - $j,k$ 为输出通道中，图像的坐标（行、列）
   - $l$ 为输入的通道（如绿色通道）
   - $m,n$ 为输入通道中，图像的坐标（行、列）

   于是局部连接层的线性部分表示为：

   $$
   Z_{i,j,k}=\sum_{l}\sum_m\sum_n[V_{l,j+m-1,k+n-1}K_{i,l,m,n,j\%t+1,t,k\%t+1}]
   $$

   > 如果是卷基层，则为 $K_{i,l,m,n}$ 

   这里百分号是取摸运算。$t$ 为不同的核的数量，输出的位置在每个方向上 $t$ 个不同的核组成的集合中循环。如果 $t$ 等于输出的宽度，这就是局部连接层

   > 因为下标从1计数，所以这里有 $-1$ 

10. 实现卷积神经网络时，为了学习模型，必须能够计算核的梯度

- 在某些简单情况下，核的梯度可以通过卷积来实现。但是大多数情况下（包括步幅大于1时），并不具有这样的性质
- 卷积是一种线性运算，所以可以表示成矩阵乘法形式。
   - 涉及的矩阵是卷积核的函数。
   - 这个矩阵是稀疏的，并且核的每个元素都复制到该矩阵的很多个位置

 假设要训练一个卷积神经网络，它包含步幅为 $s$ 的步幅卷积，卷积核为 $\mathbf K$ ，作用于多通道的图像 $\mathbf V$ 。于是卷积输出为

$$
  Z_{i,j,k}=c(\mathbf K,\mathbf V,s)_{i,j,k} =\sum_l\sum_{m=1}^{M}\sum_{n=1}^{N}V_{l,(j-1)\times s+m,(k-1)\times s+n}K_{i,l,m,n}
$$

 假设我们需要最小化某个损失函数 $J(\mathbf V,\mathbf K)$ ，则：

-  前向传播过程中，我们需要通过 $c(\cdot)$ 来计算 $\mathbf Z$  ，然后将 $\mathbf Z$ 传递到网络的其余部分，并用来计算 $J$ 

-  在反向传播过程中，我们得到一个张量 $\mathbf G$ 为： $G_{i,j,k}=\frac{\partial}{\partial Z_{i,j,k}}J(\mathbf V,\mathbf K)$ 

-  为了训练网络，我们需要对核中的权重求导。我们利用函数

   $$
   g(\mathbf G,\mathbf V,s)_{i,j,k,l}=\frac{\partial}{\partial K_{i,j,k,l}}J(\mathbf V,\mathbf K)\\
   =\sum_u\sum_m\sum_n\frac{\partial}{\partial Z_{u,m,n}}J(\mathbf V,\mathbf K)\frac{\partial Z_{u,m,n}}{\partial K_{i,j,k,l}}\\
   =\sum_m\sum_nG_{i,m,n}V_{j,(m-1)\times s+k,(n-1)\times s+l}
   $$

   最后一个等号，是因为 $Z_{i,j,k}$ 只与 $K_{i,l,m,n}$ 有关，与其他通道无关。

-  如果这一层不是网络的底层，则需要对 $\mathbf V$  求梯度来使得误差进一步反向传播：

   $$
   h(\mathbf K,\mathbf G,s)_{i,j,k}=\frac{\partial}{\partial V_{i,j,k}}J(\mathbf V,\mathbf K)\\
   =\sum_q\sum_m\sum_n\frac{\partial}{\partial Z_{q,m,n}}J(\mathbf V,\mathbf K)\frac{\partial Z_{q,m,n}}{\partial V_{i,j,k}}\\
   =\sum_{\substack{l,m \\s.t. (l-1)\times s+m=j}}\sum_{\substack{n,p\\s.t. (n-1)\times s+p=k}}\sum_qK_{q,i,m,p}G_{q,l,n} 
   $$


11. 通常在卷基层我们会加入非线性运算。我么也会在非线性单元中，对它的输入（也就是前一级的输出）加入一些偏置项

   - 对于局部连接层，每个单元都有特定的偏置
   - 对于拼贴卷积层，偏置项与核一样的拼贴模式来共享
   - 对于卷基层，通常在单元在通道级别上共享偏置（同一个通道，偏置项相同）
     - 如果输入是固定大小的，也可以在每个位置上学习一个单独的偏置。好处是：允许模型校正图像中不同位置的差异

## 6. 结构和输出

1. 卷积神经网络可以输出高维的结构化对象，而不仅仅一维的分类任务的标签或者回归任务的实数值

   - 通常这个输出对象是一个张量，由标准卷基层产生
   - 如张量 $\mathbf S$ ，其中 $S_{i,j,k}$ 是网络的输入像素 $(j,k)$ 属于类 $i$ 的概率。这允许模型标记图像中的每个像素，并绘制单个物体的精确轮廓

2. 对图像进行逐个像素标记的一种策略是：

   - 先产生图像标签的一个原始猜测
   - 然后使用相邻像素之间的校验来修正该原始猜测
   - 重复上述修正步骤相当于在每一步使用相同的卷积。很多个这样的卷积组成了一个深层网络，但是这个网络的最后几层之间存在权值连接（标准的前馈神经网络没有跨层的权值连接）
     - 这种跨层的权值连接构成了反馈，这种深层网络形成了一个特殊的循环神经网络

   ![img_output](../imgs/9/img_output.png) 

   如上图所示：输入时图像张量 $\mathbf X$， 其下标对应着图像的行、列、通道（红绿蓝）。输出是标签张量 $\hat{\mathbf Y}$ ，其下标对应着图像的行、列、不同类别的概率

   - 该网络并不是一次性输出结果，而是使用前一轮的输出来改善结果
   - 每一步使用对 $\mathbf X $ 的卷积核都是张量 $\mathbf U$ 
   - 每一步产生图像的 $\mathbf H^{(t)}$ 都需要两个输入：
     - 一个输入是通过对图像 $\mathbf X$  采用核  $\mathbf U$  来卷积
     - 一个输入是通过对前一个输出 $\hat{\mathbf Y}^{(t-1)}$ 采用核 $\mathbf W$ 进行卷积。第一次产生 $\mathbf H^{(1)}$ 时，这一项为零（因为还没有前一次输入）
   - 核张量 $\mathbf V$ 用于产生从 $\mathbf H^{(t)}$  到 $\mathbf Y^{(t)}$ 的输出

   > 因为每一步使用相同的参数，所以这是一个循环神经网络的例子

3. 一旦对每个像素都进行了一次，就可以用各种方法来进一步处理这些预测

   - 常规思路是：假设大片相连的像素对应于相同的标签。

## 7. 数据类型

1. 卷积神经网络使用的数据通常包含多个通道：每个通道都是时间/空间上一个点的某种不用的观测量

   - 同一个点，观测的角度不同，就产生了不同的通道
   - 如三维的单通道：立体成像的数据。每个点代表了三维空间的一个点。三维的多通道：彩色视频数据：时间维度+二维空间（图像）+色彩通道（红绿蓝三通道）

2. 目前我们仅讨论了数据中每个样例都具有相同的空间维度的情况。但是卷积神经网络还可以处理具有变化的空间尺度的输入（如样本集中，样本的像素可能都不相同）

   - 这种类型的输入不能使用传统的基于矩阵乘法的神经网络来表示（因为不同样本的输入的维度可能不同）

   - 但是卷积神经网络可以处理这种情况：根据输入图片的大小，核被简单的使用不同次数。并且卷积运算的输出也会相应的缩放

     > 卷积和矩阵乘法是等价的。这里可以理解为：利用卷积，卷积核为每种大小的输入引入了一个不同大小的双重循环矩阵（由卷积核构造的）

     - 如果网络的输出和输入一样的，（如为每个输入像素分配一个分类标签），则无需做更多的工作
     - 如果要求网络的输出大小是固定的，（如为整个图像分配一个类别标签），此时需要插入一个池化层，池化区域的大小要和输入的大小成比例，从而保持固定数量的池化输出

3. 卷积能处理可变大小的输入，但这种“可变”必须是因为相同事物的不同量的观察不同导致的（如：时间量上的不同观察导致时间维度可变，空间上的不同观察导致空间维度可变）

   - 这种可变并不包括特征数量的可变：比如某个输入具有 “年龄、学历、性别”特征，另一个输入只具有“年龄、学历”特征。那么卷积对于这个类型的样本集无能为力

## 8. 高效的卷积算法

1. 卷积等效于：

   - 使用傅里叶变换将输入和核都转换到频域
   - 在频域将输入和核进行逐点相乘
   - 把相乘的结果使用傅里叶逆变换转换回时域

   对于某些规模的问题，这种算法可能比离散卷积的直接计算要来的更快

2. 对于一个 $d$  维的核，如果可以表示成 $d$ 个一维向量的外积时，称该核是可分离的

   - 这里的外积不同于代数意义上的叉乘，而是：

     $$
      \mathbf{\vec a}\otimes \mathbf{\vec b}= \begin{bmatrix}a_1\\
      a_2 \\ 
      \vdots\\ 
      a_M\end{bmatrix}\otimes\begin{bmatrix}b_1 \\
      b_2 \\ 
      \vdots\\ 
      b_N\end{bmatrix}=\begin{bmatrix}a_1b_1&a_1b_2&\cdots&a_1b_N\\
      a_2b1&a_2b_2&\cdots&a_2b_N\\
      \vdots&\vdots&\ddots&\vdots\\ 
      a_Mb_1&a_Mb_2&\cdots&a_Mb_N\end{bmatrix}
     $$

   - 当核可分离时，直接计算卷积是非常低效的。它等价于组合 $d$ 个一维向量的卷积，每次卷积时使用这些向量中的一个

     - 如果核在每个维度都是 $w$ 个元素宽，那么直接计算卷积需要 $O(w^d)$  的运行时间和空间（存储参数），而可分离的卷积只需要 $O(w\times d)$ 的运行时间和参数存储空间

   - 如果核可分离时，采用可分离的卷积效果更好。但是并不是每个核都是可分离的

3. 设计更快的卷积或者近似卷积而不降低模型准确率的方法是一个活跃的研究领域

   - 甚至仅提高前向传播效率的技术也是有用的。因为在商业环境中，通常部署网络比训练网络还要耗资源

## 9. 随机或者无监督的特征

1. 通常卷积网络训练中最昂贵的部分是学习特征

   - 输出层通常相对廉价。因为经过若干层池化之后，输出层的输入的特征的数量就相对少得多

     > 这里的学习特征，应该是学习卷积核。每一个单元的输入都是特征，但是一个单元的输入也是另一个单元的输出（除了最原始的的输入 $\mathbf X$） 。因此学习特征就是要学得中间每一级的输出。其中最重要的就是卷基层的输出

2. 当使用梯度下降执行有监督训练时，每个梯度步骤需要完整的在全网中运行前向传播和反向传播

   - 减少训练成本的方法是：使用那些非监督方式训练的特征

3. 有三种基本策略来避免监督训练而得到卷积核

   - 第一种方法是：简单地随机初始化卷积核
   - 第二种方法是：手动设计一个卷积核。如设计一个检测图像边缘的核
   - 第三种方法是：用无监督来学习核

4. 使用无监督来学习核时，允许其结构与网络顶层的分类曾相分离

   - 使用无监督学习特征
   - 学到之后就提取训练集的全部特征，构建一个新的训练集
   - 将这个新的训练集作为最后一层的输入

5. 随机过滤器经常在卷积网络中表现的出乎意料的好

   - 通常给出多个随机权值，生成一组候选的卷积网络结构
   - 然后仅仅训练最后一层来评估几个卷积网络结构的性能，挑选最好的结构，并且使用代价更大的方法来训练整个网络

6. 还有一种办法是使用一些特殊的方法来学习特征，但是这些方法不需要再每个梯度步骤中都完整的前向和反向传播

   - 比如：使用贪心逐层式预训练：
     - 独立地训练第一层。然后从第一层提取所有特征构建新的训练集
     - 然后用新训练集独立地训练第二层...以此类推
   - 卷积模型的贪心逐层预训练的经典模型是卷积深度信念网络
   - 卷积网络能够为我们提供相对于多层感知机更进一步的预训练的机会：不是一层训练整个卷基层，而是训练一小块模型。然后使用这个小块模型的参数来定义卷基层的核。
     - 这意味着使用无监督学习来训练卷积网络，并且在训练过程中完全不使用卷积也是可能的
     - 使用这种方法，我们可以训练非常大的模型，并且只在推理期间产生高计算成本

7. 如今大多数卷积网络以纯粹有监督的方式训练，因为计算能力比以前大幅度提升

8. 无监督预训练的好处难以说清

   - 使用无监督预训练可以提供一些相对于监督训练的正则化
   - 使用无监督预训练可以训练更大的结构，因为它的学习规则减少了计算成本

## 10. 卷积神经网络的神经科学基础

1. 神经生理学家`David Hubel`核`Torsten Wiesel`观察了猫的脑内神经元的视觉响应：处于视觉系统较为前面的神经元对于特定的光模式反应最强烈，但是对于其它模式几乎完全没有反应

   对这一过程进行简化：我们关注大脑的 $V1$  部分，也称作主要视觉皮层。

   - 图像从光到达眼睛并刺激视网膜
   - 视网膜中的神经元对图像进行一些简单的预处理，但是基本不改变图像的表达方式
   - 图像通过视神经，以及称作外侧膝状体的脑部区域

2. 卷积层被设计为描述 $V1$ 的三个性质：

   - $V1$ 分布在空间图中。它实际上具有二维结构来映射视网膜中的图像

     - 视网膜下半部的光仅仅影响 $V1$ 相应的一半
     - 卷积网络通过用二维映射定义特征的方式来描述该特性

   - $V1$ 包含许多简单细胞。这些简单细胞的行为简单概括为：在一个空间上小的、局部的接受域内的图像的线性函数

     - 卷积网络的检测器单元被设计为模拟简单细胞

   - $V1$ 还包括许多复杂细胞。这些细胞相应类似于由简单细胞检测的那些特征。

     - 但是复杂细胞对于特征的位置的微小偏移具有不变性（这通过卷积网络的池化单元来刻画）

     - 复杂细胞对于照明中的一些变化也是不变的（它不能简单地通过在空间位置上池化来刻画）

       > 这些不变性激发了卷积网络中的一些跨通道池化策略，如`maxout`单元

3. 一般认为：类似于 $V1$ 的原理也适用于视觉系统的其他区域

   - 在大脑中，我们找到了响应一些特定概念的细胞，并且这种细胞对于输入的许多种变换都具有不变性。这些细胞被称作祖母细胞

     > 一个人可能有这样的一个神经元，当他看到祖母的照片时，该神经元被激活。无论祖母出现在照片的哪个位置、无论是祖母的脸部还是全身、无论是光亮还是黑暗

   - 这些祖母细胞存在于内侧颞叶的区域

4. 与卷积网络最后一层特征上最接近的类比是：颞下皮质的脑区

   - 当查看一个对象时，信息从视网膜经过 `LGN` 流到 $V1$ ，然后到 $V2$ ，$V4$ ，然后是颞下皮质
   - 这发生在瞥见对象的前 100ms 内
   - 如果允许一个人继续观察对象更多的时间，那么信息将开始向后流动（即前面过程的反馈路径）。因为大脑使用自上而下的反馈来更新较低级脑区中的激活
   - 如果打断人的注视，并且只观察前 100ms 内的大多数前向传播路径，则颞下皮质与卷积网络非常相似

5. 动物的视觉系统与卷积网络的主要区别：

   - 人眼大部分是非常低的分辨率，除了一个被称作中央凹的小块（手臂远的拇指大小的区域）
     - 虽然我们觉得我们可以看到高分辨率的整个场景，但是这是大脑的潜意识部分创建的错觉，因为它缝合了我们瞥见的若干个小区域。大多数卷积网络实际上接收到的是一张高分辨率的照片
     - 人类大脑控制几次眼动（称作扫视），从而瞥见场景中最显眼的或者任务相关的部分。这称作关注机制。目前关注机制对于自然语言处理是最成功的
   - 人类视觉系统集成了许多其他感觉（如听觉，以及心情想法之类的因素）。卷及网络目前为止纯粹是视觉的
   - 人类视觉系统不仅用于识别对象，它还能够理解整个场景：包括许多对象、对象之间的关系、我们的身体与世界交互所需要的丰富的三维几何信息。卷积神经网络在这些问题上还是起步阶段
   - 即使像 $V1$ 这样简单的大脑区域也受到来自较高级别的反馈的严重影响。虽然神经网络模型也探索反馈机制，但是目前没有提供引人瞩目的改进
   - 大脑可能使用非常不同的激活核池化函数。单个神经元的激活可能并不能通过单个线性过滤器的相应来很好的表征。

6. 对大脑的研究表明，大多数的 $V1$ 细胞具有由 `Gabor`函数描述的权重。

   - 我们认为图像是由 $I(x,y)$ 描述的函数

   - 我们认为简单细胞是在图像中的某些位置上进行采样。

     - 这些位置由一组 $x$ 坐标 $\mathbb X$  核一组 $y$ 坐标 $\mathbb Y$  来定义。

     - 采样的结果的权重为 $w(x,y)$ 

     - 简单细胞对于图像的响应为：

       $$
        s(I)=\sum_{x\in \mathbb X}\sum_{y\in \mathbb Y}w(x,y)I(x,y)
       $$

7. `Gabor` 函数描述了图像中的 2维点处的权重

     $$
      w(x,y;\alpha,\beta_x,\beta_y,f,\phi,x_0,y_o,\tau)=\alpha \exp(-\beta_x x^{\prime2}-\beta_yy^{\prime2})\cos(fx^{\prime}+\phi) 
     $$

     其中

     $$
      x^{\prime}=(x-x_0)\cos(\tau)+(y-y_0)\sin(\tau)\\ y^{\prime}=-(x-x_0)\cos(\tau)+(y-y_0)\sin(\tau)
     $$

     这里 $\alpha,\beta_x,\beta_y,f,\phi,x_0,y_o,\tau$ 都是控制 `Gabor`函数性质的参数

     - 参数 $x_0,y_0,\tau$ 定义坐标系。

       - 我们通过平移和旋转 $x,y$ 来得到 $x^{\prime},y^{\prime}$。 简单细胞会响应以点 $(x_0,y_0)$ 为中心的图像特征。
       - 当我们沿着水平方向旋转 $\tau$  弧度时， 简单细胞将响应亮度的变化。
       - 下图中：白色表示较大的正权重；黑色表示较大的负权重；灰色对应零权重。网格中每个`Gabor`函数被赋予它在网格中的位置成比例的 $x_0,y_0$ 的值。$\tau$ 的值被选择为：`Gabor`的值对径向非常敏感。其他参数固定不变

       ![response_translate](../imgs/9/response_translate.png) 

       -  当我们沿着 $w^{\prime}$ 移动时，函数 $w$ 通过亮度变换来响应移动。它有两个重要的因子：一个是高斯函数，另一个是余弦函数。

     - $\alpha,\beta_x,\beta_y$ 调整高斯因子

       - 高斯因子（高斯函数） $\alpha \exp(-\beta_x x^{\prime2}-\beta_yy^{\prime2})$ 可以视作阈值项，用于保证简单细胞仅对接近 $x^{\prime}=0,y^{\prime}=0$ 处的值产生响应。即：只有在中心点处产生较强的响应，远离中心点则响应指数级衰减尺度因子 

       - $\alpha$  调整简单细胞响应的总量级

       - $\beta_x,\beta_y$ 控制接受域衰减的速度（即远离中心点的衰减速度）

       - 下图中：白色表示较大的正权重；黑色表示较大的负权重；灰色对应零权重。$x_0,y_0,\tau$ 固定为零。从左到右， $\beta_x$ 减少；从上到下， $\beta_y$ 减少。其他参数保持不变

         ![response_gaussian](../imgs/9/response_gaussian.png)

     - $f,\phi$  控制余弦因子

       - 余弦因子（余弦函数） $\cos (fx^{\prime}+\phi)$ 控制简单细胞如何响应沿着 $x^{\prime}$ 轴的变化

       - 参数 $f$ 控制余弦的频率

       - 参数 $\phi$ 控制它的相位偏移

       - 下图中：白色表示较大的正权重；黑色表示较大的负权重；灰色对应零权重。$x_0,y_0,\tau$ 固定为零。从左到右， $\phi$ 增加；从上到下， $f$ 增加。其他参数保持不变

         ![response_cos](../imgs/9/response_cos.png)

     - 合在一起的意义为：简单细胞对特定位置处、特定方向上、特定空间频率的亮度进行响应

       - 当图像中的光波与细胞的权重具有相同的相位时，简单细胞时最活跃的：当图像亮时，权重为正；图像暗时，权重为负
       - 当光波与权重完全异相时，简单细胞被抑制的：当图像为亮时，权重为负；图像暗时，权重为正

8. 复杂细胞是计算它包含两个简单细胞响应的 2维向量的 $L^{2}$ 范数

     $$
      c(I)=\sqrt{s_0(I)^{2}+s_1(I)^{2}}
     $$

     - 一个重要特殊情况是：当 $s_1,s_0$ 除了 $\phi$ 以外的参数都相同； 且 $\phi$  参数上， $s_0,s_1$ 相位相差 $\frac 14$ 个周期时， $s_0,s_1$ 形成正交对
     - 复杂细胞对于图像在方向 $\tau$  上的微小变换或者翻转图像（黑白互换）具有不变性


## 11. 卷积神经网络与深度学习的历史

1. 卷积神经网络时第一个解决重要商业应用的神经网络
2. 卷积网络是用反向传播训练的第一个有效的深度网络之一
3. 卷积网络提供了一种方法来专业化神经网络，从而处理具有清楚的网络结构拓扑的数据
   - 这种方法在二维图像拓扑上是最成功的
   - 为了处理一维序列数据，我们往往采用另一种强大的专业化网络：循环神经网络












